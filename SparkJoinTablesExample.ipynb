{
 "metadata": {
  "name": "",
  "signature": "sha256:5781621d509f4566ebaa670f7ee9fa8c6d301db879277ee40922622f4095d9fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Some Spark SQL...\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "#import sys\n",
      "import glob\n",
      "import re\n",
      "import math\n",
      "import matplotlib.pyplot as plt # http://matplotlib.org/users/customizing.html\n",
      "#import operator\n",
      "from operator import add\n",
      "#from pyspark import SparkContext, SparkConf\n",
      "from pyspark.sql import SQLContext, Row\n",
      "from pyspark.sql.types import *\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "spark_home = os.environ.get('SPARK_HOME', None)\n",
      "print('SPARK initialized OK! The SPARK home directory is: ' + spark_home)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SPARK initialized OK! The SPARK home directory is: c:\\spark\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
      "train_set = sc.textFile('C:/coding/Kaggle/CaterpillarTubePricing/Data/data/competition_data/train_set_no_header.csv')\n",
      "tube = sc.textFile('C:/coding/Kaggle/CaterpillarTubePricing/Data/data/competition_data/tube_no_header.csv')\n",
      "\n",
      "# Skip header line first (works OK in shell):\n",
      "# header_line = train_set.map(lambda x: x).first()\n",
      "# train_set2 = train_set.filter(lambda x: x.startswith('TA-'))\n",
      "train_set_data = (train_set.map(lambda l: l.split(\",\")).map(lambda p: [p[0],p[1],p[2],int(p[3]),int(p[4]),p[5],\n",
      "                                                                       int(p[6]),float(p[7])]))\n",
      "tube_data = (tube.map(lambda l: l.split(\",\")).map(lambda p: [p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8],p[9],\n",
      "                                                            p[10],p[11],p[12],p[13],p[14],p[15]]))\n",
      "\n",
      "tube_header = 'tube_assembly_id,material_id,diameter,wall,length,num_bends,bend_radius,' + \\\n",
      "    'end_a_1x,end_a_2x,end_x_1x,end_x_2x,end_a,end_x,num_boss,num_bracket,other'\n",
      "\n",
      "schema = (StructType([\n",
      "    StructField(\"tube_assembly_id\", StringType(), True),\n",
      "    StructField(\"supplier\", StringType(), True),\n",
      "    StructField(\"quote_date\", StringType(), True),\n",
      "    StructField(\"annual_usage\", IntegerType(), True),\n",
      "    StructField(\"min_order_quantity\", IntegerType(), True),\n",
      "    StructField(\"bracket_pricing\", StringType(), True),\n",
      "    StructField(\"quantity\", IntegerType(), True),\n",
      "    StructField(\"cost\", FloatType(), True)\n",
      "]))\n",
      "\n",
      "fields = [StructField(field_name, StringType(), True) for field_name in tube_header.split(',')]\n",
      "tube_schema = StructType(fields)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tube_data.filter(lambda p: float(p[2]) > 155.0).collect()\n",
      "\n",
      "train_set_data_df = sqlContext.createDataFrame(train_set_data, schema)\n",
      "# train_set_data_df.show()\n",
      "tube_data_df = sqlContext.createDataFrame(tube_data, tube_schema)\n",
      "\n",
      "# Register the DataFrame as a table\n",
      "train_set_data_df.registerTempTable('train_set')\n",
      "tube_data_df.registerTempTable('tube')\n",
      "\n",
      "# SQL can be run over DataFrames that have been registered as a table\n",
      "result = sqlContext.sql('SELECT * FROM train_set WHERE cost > 33.33 ORDER BY cost DESC')\n",
      "result.show()\n",
      "# The results of SQL queries are RDDs and support all the normal RDD operations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 6, localhost): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:159)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:595)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:415)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)\r\n\tat org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1771)\r\n\tat org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1484)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1445)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-f07c98adea7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtube_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m155.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_set_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# train_set_data_df.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtube_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtube_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtube_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# take the first few rows to verify schema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;31m# Row() cannot been deserialized by Pyrolite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Row'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\spark\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[1;32m--> 891\u001b[1;33m                                           allowLocal)\n\u001b[0m\u001b[0;32m    892\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.8.2.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.8.2.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 6, localhost): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:159)\r\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:595)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:415)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)\r\n\tat org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1771)\r\n\tat org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1484)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1445)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# https://github.com/databricks/spark-csv\n",
      "# csv_file = 'C:/coding/Kaggle/CaterpillarTubePricing/Data/data/competition_data/train_set.csv'\n",
      "# df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load(csv_file)\n",
      "# df.show()\n",
      "# df.select('year', 'model').write.format('com.databricks.spark.csv').save('newdata.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO:\n",
      "result = sqlContext.sql(\"SELECT * FROM train_set t1 JOIN tube t2 ON t1.tube_assembly_id=t2.tube_assembly_id\").collect\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}